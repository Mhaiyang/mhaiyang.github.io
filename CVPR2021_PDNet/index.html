<!DOCTYPE html>
<html>
<head>
<title>CVPR2021_PDNet</title>

<style media="screen" type="text/css">
body
{
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}
body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 1080px;
  font-family: Times New Roman, Lato, Verdana, Helvetica, sans-serif;
  font-size: 14px;
  background: #fdfdfd;
}
</style>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?1ec4ad5c61857459aa78d5ee7ddee28d";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>
	<script src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.8.0.js"></script>
</head>

<body>
<h3 align="center"><i><font size="3" face="Palatino Linotype">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2021</font></i></h3>

<table align="center">
<td align="center">
<h1>Depth-Aware Mirror Segmentation</h1>
<h3>
	<a href="http://mhaiyang.github.io" target="_blank"><font size="3"><b>Haiyang Mei</b></font></a><sup><font size="2">1</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
	<a href="https://dongshuhao.github.io/" target="_blank"><font size="3"><b>Bo Dong</b></font></a><sup><font size="2">2</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
	<font size="3"><b>Wen Dong</b></font><sup><font size="2">1</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
	<a href="http://www.cs.wm.edu/~ppeers/" target="_blank"><font size="3"><b>Pieter Peers</b></font></a><sup><font size="2">3</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
	<a href="https://xinyangdut.github.io/" target="_blank"><font size="3"><b>Xin Yang</b></font></a><sup><font size="2">1,*</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
	<br>
	<font size="3">Qiang Zhang</font><sup><font size="2">1</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
	<font size="3">Xiaopeng Wei</font><sup><font size="2">1</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
</h3>

<sup><font size="2">1</font></sup>
<b><a><font size="3">Dalian University of Technology</font></a></b>&nbsp;&nbsp;&nbsp;&nbsp;
<sup><font size="2">2</font></sup>
<b><a><font size="3">SRI International</font></a></b>&nbsp;&nbsp;&nbsp;&nbsp;
<sup><font size="2">3</font></sup>
<b><a><font size="3">College of William & Mary</font></a></b>

<!--<br>-->
<!--<br>&nbsp;-->
<!--&lt;!&ndash;<sup><font size="2">&dagger;</font></sup>&ndash;&gt;-->
<!--<sup><font size="3">*</font></sup>-->
<!--<a><font size="3"> Corresponding author</font></a>-->
<br>
<br>&nbsp;

	<b><a><font size="3"> Contact us:&nbsp;&nbsp;&nbsp;&nbsp;<i>xinyang@dlut.edu.cn&nbsp;&nbsp;&nbsp;&nbsp;mhy666@mail.dlut.edu.cn</i></font></a></b>

</td>
</table>


<br><br>
<table align="center">
<tr>
	<td align="center"><img border=0 height=450 width=900 src="mirror.gif"></td>
</tr>
</table>

<!--<br>-->
<!--<table align="center">-->
<!--<tr>-->
	<!--<td align="center"><img border=0 height=350 width=800 src="Glass.gif"></td>-->
<!--</tr>-->
<!--</table>-->

<br>
<h2><p><font size="6"><b>Abstract</b></font></p></h2>
<hr/>
<p><font size="4" face="Palatino Linotype">We present a novel mirror segmentation method that leverages depth estimates from ToF-based cameras as an additional cue to disambiguate challenging cases where the contrast or relation in RGB colors between the mirror re-flection and the surrounding scene is subtle. A key observation is that ToF depth estimates do not report the true depth of the mirror surface, but instead return the total length ofthe reflected light paths, thereby creating obvious depth dis-continuities at the mirror boundaries. To exploit depth information in mirror segmentation, we first construct a large-scale RGB-D mirror segmentation dataset, which we subse-quently employ to train a novel depth-aware mirror segmentation framework. Our mirror segmentation framework first locates the mirrors based on color and depth discontinuities and correlations. Next, our model further refines the mirror boundaries through contextual contrast taking into accountboth color and depth information. We extensively validate our depth-aware mirror segmentation method and demonstrate that our model outperforms state-of-the-art RGB and RGB-D based methods for mirror segmentation. Experimental results also show that depth is a powerful cue for mirror segmentation.
</font></p>


<!--<br>-->
<!--<h2><p><font size="6"><b>PDNet</b></font></p></h2>-->
<!--<hr/>-->
<!--<table align="center">-->
<!--<tr>-->
	<!--&lt;!&ndash;<td align="center"><img border=0 height=270 width=960 src="pipeline.png"></td>&ndash;&gt;-->
<!--</tr>-->
<!--</table>-->

<!--<br>-->
<!--<h2><p><font size="6"><b>Visual Results</b></font></p></h2>-->
<!--<hr/>-->
<!--<table align="center">-->
<!--<tr>-->
	<!--&lt;!&ndash;<td align="center"><img border=0 height=300 width=900 src="CVPR2020_Glass_1.gif"></td>&ndash;&gt;-->
<!--</tr>-->
<!--</table>-->


<br>
<h2><p><font size="6"><b>Downloads</b></font></p></h2>
<hr/>
<div align="left">
		<table>
		<tr align="left">
		<td>
			<font size="4">Paper</font>
		</td>
		<td>
			<font size="4">: [ <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Mei_Depth-Aware_Mirror_Segmentation_CVPR_2021_paper.pdf" target="_blank">PDNet.pdf</a> ]</font>
		</td>
		</tr>

		<!--<tr align="left">-->
		<!--<td>-->
			<!--<font size="4">Glass Detection Dataset</font>-->
		<!--</td>-->
		<!--<td>-->
		<!--<font size="4">: [ Google Drive ]</font>-->
		<!--&lt;!&ndash;<font size="4">: <a href="" target="_blank">[ Google Drive ]</a></font>&ndash;&gt;-->
		<!--</td>-->
		<!--</tr>-->

		<tr align="left">
		<td>
			<font size="4">Experimental results</font>
		</td>
		<td>
			<font size="4">: [ <a href="https://drive.google.com/file/d/1GP1_rm6FSRKVJ1AxV-DSquKjXtpNVF32/view?usp=sharing" target="_blank">Google Drive</a> ]</font>
			<font size="4">[ <a href="https://pan.baidu.com/s/1rFVaXeH3Ob1B5VfoeYLgZw" target="_blank">Baidu Disk</a>, fetch cod: rrr3 ]</font>
		</td>

		</tr>

		<tr align="left">
		<td>
			<font size="4">Pre-trained model</font>
		</td>
		<td>
			<font size="4">: [ <a href="https://drive.google.com/file/d/1gxXuDiZieu0z3yUC6AC6FT3DuMKh8ZNL/view?usp=sharing" target="_blank">Google Drive</a> ]</font>
			<font size="4">[ <a href="https://pan.baidu.com/s/1bZpZ6_zv_zPzxxiRlWOljw" target="_blank">Baidu Disk</a>, fetch cod: rtf4 ]</font>
		</td>
		</tr>

		<tr align="left">
		<td>
			<font size="4">Source code</font>
		</td>
		<td>
			<!--<font size="4">: [ Code ]</font>-->
			<font size="4">: [ <a href="https://github.com/Mhaiyang/CVPR2021_PDNet" target="_blank">Code</a> ] </font>
		</td>
		</tr>

		</table>
</div>
<br>
<br>


<h2><p><font size="6" color="black"><b>Dataset</b></font></p></h2>
<hr/>

<font size="4"> Both <font size="4" color="red">training set</font> and <font size="4" color="red">testing set</font> can be obtained via form request! </font>
<br><br>

<font size="3">
	To request access to the dataset for non-commercial use, please review the terms and conditions. If you agree with them, please fill the form below and then click the "<font color="black">Send Request</font>" button to achieve a request. Please fill in your official university/company email address. Thank you!

	<br>
	<br>

	<b>Terms and Conditions</b>
	<br>

The dataset can be used freely if you agree with all the following terms.<br>

 - The dataset is used only for non-commercial purposes, such as teaching and research. You do not use the dataset or any of its modified versions for any purposes of commercial advantage or private financial gain.<br>
 - You do not distribute the dataset or any of its modified versions to other individuals, institutes, companies, associations or public.<br>
 - In case you use the dataset within your research papers, you refer to our publications on our website. If the dataset is used in media, a link to our website is included.<br>
 - We reserve all rights that are not explicitly granted to you. The dataset is provided as is, and you take full responsibility for any risk of using it. There may be inaccuracies although we tried, and will try our best to rectify any inaccuracy once found.

</font>
<br>

<h3>Sending Request to Prof. Xin Yang (xinyang@dlut.edu.cn):</h3>

<form class="form" id="emailForm">
	Name:<br>
    <input id="first" name='name' type="text" placeholder="Your name..." class="form__input" />
	<br><br>
	Institute:<br>
    <input id="second" name='institute' type="text" placeholder="Your institute..." class="form__input" />
	<br><br>
	E-mail:<br>
	<input id="third" name='email' type="text" placeholder="Your E-mail address..." class="form__input" />
    <!--<textarea id="third" name='e-mail' type="text" placeholder="Your E-mail..." class="form__input"></textarea>-->
</form>
<br>
<button id="btnSubmit">Send Request</button>
<br><br><br><br>


<!--<br><br><br><br>-->
<!--<h2><p><font size="6" color="black"><b>Report</b></font></p></h2>-->
<!--<hr/>-->
<!--<table align="center">-->
<!--<tr>-->
	<!--<td align="center"><embed src="report.mp4" width="800" height="450"></td>-->
<!--</tr>-->
<!--</table>-->


<h2><p><font size="6" color="black"><b>BibTex</b></font></p></h2>
<hr/>
<font size="3">
@InProceedings{Mei_2021_CVPR_PDNet,<br>
&nbsp;&nbsp;&nbsp;&nbsp;author = {Mei, Haiyang and Dong, Bo and Dong, Wen and Peers, Pieter and Yang, Xin and Zhang, Qiang and Wei, Xiaopeng},<br>
&nbsp;&nbsp;&nbsp;&nbsp;title = {Depth-Aware Mirror Segmentation},<br>
&nbsp;&nbsp;&nbsp;&nbsp;booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},<br>
&nbsp;&nbsp;&nbsp;&nbsp;month = {June},<br>
&nbsp;&nbsp;&nbsp;&nbsp;year = {2021}<br>
}
</font>


<br><br>
<h2>Website visit statistics</h2>
<hr/>
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=400&t=tt&d=EsUkw_m2bHmFxftVUNklQ6CouRkNwbDnlRJFNuQiqBU&co=2d78ad&cmo=3acc3a&cmn=ff5353&ct=ffffff'></script>


<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/emailjs-com@2/dist/email.min.js"></script>
<script type="text/javascript">
   (function(){
      emailjs.init("user_objBJSsyEdqmhf8JaUiLu");
   })();
</script>
<script type="text/javascript" src="./main.js"></script>

</body>

</html>
