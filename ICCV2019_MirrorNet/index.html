<!DOCTYPE html>
<html>
<head>
<title>ICCV2019_MirrorNet</title>

<style media="screen" type="text/css">
body
{
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}
body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 1080px;
  font-family: Times New Roman, Lato, Verdana, Helvetica, sans-serif;
  font-size: 14px;
  background: #fdfdfd;
}
</style>
</head>

<body>

<table align="center">
<td align="center">
<h1>Where is My Mirror?</h1>
</td>
</table>

<br>
<br>
<table align="center">
<tr>
	<td align="center"><img border=0 height=300 width=720 src="teaser_top.jpg"></td>
</tr>
<tr>
	<td align="center"><img border=0 height=300 width=720 src="teaser_bottom.jpg"></td>
</tr>
<tr>
	<td align="center"> <br><br> </td>
</tr>
<tr>
	<td align="center"><img border=0 height=270 width=960 src="pipeline.jpg"></td>
</tr>
</table>


<br>
<h2><p><font size="6"><b>Abstract</b></font></p></h2>
<hr/>
<p><font size="5" face="Palatino Linotype">Mirrors are everywhere in our daily lives. Existing computer vision systems do not consider mirrors, and hence may get confused by the reflected content inside a mirror, resulting in a severe performance degradation. However, separating the real content outside a mirror from the reflected content inside it is non-trivial.  The key challenge lies in that mirrors typically reflect contents similar to their surroundings, making it very difficult to differentiate the two. In this paper, we present a novel method to accurately segment mirrors from an input image. To the best of our knowledge, this is the first work to address the mirror segmentation problem with a computational approach. We make the following contributions. First, we construct a large-scale mirror dataset that contains mirror images with the corresponding manually annotated masks. This dataset covers a variety of daily life scenes, and will be made publicly available for future research. Second, we propose a novel network, called MirrorNet, for mirror segmentation, by modeling both semantical and low-level color/texture discontinuities between the contents inside and outside of the mirrors. Third, we conduct extensive experiments to evaluate the proposed method, and show that it outperforms the carefully chosen baselines from the state-of-the-art detection and segmentation methods.
</font></p>


<br>
<h2><p><font size="6"><b>Downloads</b></font></p></h2>
<hr/>
<div align="left">
		<table>						
		<tr align="left">
		<td>
		<font size="5"><a href="https://arxiv.org/abs/1804.04371">[MirrorNet.pdf]</font></a>
		</td>
		<td>
			<font size="5">: The paper.</font>
		</td>	
		</tr>	
							
		<tr align="left">
		<td>
		<font size="5"><a href="https://drive.google.com/file/d/1qLWVostJxDGW4udVz4QDyL3126mIbCJG/view?usp=sharing">[Results.zip]</font></a>
		</td>
		<td>
			<font size="5">: Experimental results.</font>
		</td>	
		</tr>
					
		<tr align="left">
		<td>
		<font size="5"><a href="https://github.com/mhaiyang/ICCV2019_MirrorNet">[Code]</font></a>
		</td>
		<td>
			<font size="5">: Source code.</font>
		</td>	
		</tr>	
			
			
		</table>
</div>
<br>
<br>


<h2><p><font size="6" color="black"><b>BibTex</b>&nbsp(DOI)</p></h2>
<hr/>								
<font size="5">
@inproceedings{yang2019mirrornet,<br>
&nbsp;&nbsp;&nbsp;&nbsp;author = {Yang, Xin and Mei, Haiyang and Xu, Ke and Wei, Xiaopeng and Yin, Baocai and Rynson, Lau},<br>
&nbsp;&nbsp;&nbsp;&nbsp;title = {Where is My Mirror?},<br>
&nbsp;&nbsp;&nbsp;&nbsp;booktitle = {IEEE International Conference on Computer Vision},<br>
&nbsp;&nbsp;&nbsp;&nbsp;year = {2019},<br>
&nbsp;&nbsp;}
</font>


</body>

</html>
