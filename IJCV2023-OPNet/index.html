<!DOCTYPE html>
<html>
<head>
<title>IJCV2023_OPNet</title>

<style media="screen" type="text/css">
body
{
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}
body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 1080px;
  font-family: Times New Roman, Lato, Verdana, Helvetica, sans-serif;
  font-size: 14px;
  background: #fdfdfd;
}
</style>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?1ec4ad5c61857459aa78d5ee7ddee28d";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>
	<meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
	<script src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.8.0.js"></script>
</head>

<body>
<h3 align="center"><i><font size="3" face="Palatino Linotype">International Journal of Computer Vision (IJCV) 2023</font></i></h3>

<table align="center">
<td align="center">
<h1>Camouflaged Object Segmentation with Omni Perception</h1>
<h3>
	<a href="http://mhaiyang.github.io" target="_blank"><font size="3"><b>Haiyang Mei</b></font></a><sup><font size="2">1</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
	<a href="https://kkbless.github.io/" target="_blank"><font size="3"><b>Ke Xu</b></font></a><sup><font size="2">2</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
	<font size="3"><b>Yunduo Zhou</b></font><sup><font size="2">1</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
	<font size="3"><b>Yang Wang</b></font><sup><font size="2">1</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
	<font size="3"><b>Haiyin Piao</b></font><sup><font size="2">3</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
	<font size="3"><b>Xiaopeng Wei</b></font><sup><font size="2">1</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
	<a href="https://xinyangdut.github.io/" target="_blank"><font size="3"><b>Xin Yang</b></font></a><sup><font size="2">1,*</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
	<br>
</h3>

<sup><font size="2">1</font></sup>
<b><a><font size="3">Dalian University of Technology</font></a></b>&nbsp;&nbsp;&nbsp;&nbsp;
<sup><font size="2">2</font></sup>
<b><a><font size="3">City University of Hong Kong</font></a></b>&nbsp;&nbsp;&nbsp;&nbsp;
<sup><font size="2">3</font></sup>
<b><a><font size="3">Northwestern Polytechnical University</font></a></b>&nbsp;&nbsp;&nbsp;&nbsp;

<!--<br>-->
<!--<br>&nbsp;-->
<!--&lt;!&ndash;<sup><font size="2">&dagger;</font></sup>&ndash;&gt;-->
<!--<sup><font size="3">*</font></sup>-->
<!--<a><font size="3"> Corresponding author</font></a>-->
<br>
<br>&nbsp;

	<b><a><font size="3"> Contact us:&nbsp;&nbsp;&nbsp;&nbsp;<i>haiyang.mei@outlook.com&nbsp;&nbsp;&nbsp;&nbsp;xinyang@dlut.edu.cn</i></font></a></b>

</td>
</table>

<br><br>
<table align="center">
<tr>
	<td align="center"><embed height="300" width="600" src="teaser.png"></td>
</tr>
</table>

<br>
<!--<h2><p><font size="6"><b>Abstract</b></font></p></h2>-->
<hr/>

<h4><p><font size="5" color="black"><b>1. Abstract</b></font></p></h4>
<font size="4" face="Palatino Linotype">Camouflaged object segmentation (COS) is a very challenging task due to the deceitful appearances of the candidate objects to the noisy backgrounds. Most existing state-of-the-art methods mimic the first-positioning-then-focus mechanism of predators, but still fail in positioning camouflaged objects in cluttered scenes or delineating their boundaries. The key reason is that their methods do not have a comprehensive understanding of the scene when they spot and focus on the objects, so that they are easily attracted by local surroundings. An ideal COS model should be able to process local and global information at the same time, i.e., to have omni perception of the scene through the whole process of camouflaged object segmentation. To this end, we propose to learn the omni perception for the first-positioning-then-focus COS scheme. Specifically, we propose an omni perception network (OPNet) with two novel modules, i.e., the pyramid positioning module (PPM) and dual focus module (DFM). They are proposed to integrate local features and global representations for accurate positioning of the camouflaged objects and focus on their boundaries, respectively. Extensive experiments demonstrate that our method, which runs at 54 fps, significantly outperforms 15 cutting-edge models on 4 challenging datasets under 4 standard metrics.
</font>

<br>
<h4><p><font size="5"><b>2. Downloads</b></font></p></h4>
<!--<hr/>-->
<div align="left">
		<table>
		<tr align="left">
		<td>
			<font size="4">Paper</font>
		</td>
		<td>
			<font size="4">: [ <a href="https://link.springer.com/article/10.1007/s11263-023-01838-2" target="_blank">OPNet.pdf</a> ]</font>
<!--			<font size="4">: [ PGSNet.pdf ]</font>-->
		</td>
		</tr>

		<!--<tr align="left">-->
		<!--<td>-->
			<!--<font size="4">Glass Detection Dataset</font>-->
		<!--</td>-->
		<!--<td>-->
		<!--<font size="4">: [ Google Drive ]</font>-->
		<!--&lt;!&ndash;<font size="4">: <a href="" target="_blank">[ Google Drive ]</a></font>&ndash;&gt;-->
		<!--</td>-->
		<!--</tr>-->

		<tr align="left">
		<td>
			<font size="4">Experimental results</font>
		</td>
		<td>
			<font size="4">: [ <a href="https://drive.google.com/drive/folders/1FBvSmvjsdlUX6wJoJvsJymkQOu3oEz5F?usp=share_link" target="_blank">Google Drive</a> ]</font>
			<font size="4">[ <a href="https://1drv.ms/u/s!Aj8NVbiQ19n2gugBfIOrPFyTKe458A?e=f1Xm1d" target="_blank">OneDrive</a> ]</font>
			<font size="4">[ <a href="https://pan.baidu.com/s/1JUiGIuRZdMAR2asHDGDZCw?pwd=omni" target="_blank">Baidu Disk</a>, fetch code: omni ]</font>
		</td>

		</tr>

		<tr align="left">
		<td>
			<font size="4">Pre-trained model</font>
		</td>
		<td>
			<font size="4">: [ <a href="https://drive.google.com/drive/folders/1FBvSmvjsdlUX6wJoJvsJymkQOu3oEz5F?usp=share_link" target="_blank">Google Drive</a> ]</font>
			<font size="4">[ <a href="https://1drv.ms/u/s!Aj8NVbiQ19n2grZ1XRf-GWhD6A2cNA?e=Q6O6hR" target="_blank">OneDrive</a> ]</font>
			<font size="4">[ <a href="https://pan.baidu.com/s/1JUiGIuRZdMAR2asHDGDZCw?pwd=omni" target="_blank">Baidu Disk</a>, fetch code: omni ]</font>
		</td>
		</tr>

<!--		<tr align="left">-->
<!--		<td>-->
<!--			<font size="4">E2PD Dataset</font>-->
<!--		</td>-->
<!--		<td>-->
<!--			<font size="4">: [ <a href="" target="_blank">Google Drive</a> ]</font>-->
<!--			<font size="4">[ <a href="" target="_blank">Baidu Disk</a>, fetch code: omni ]</font>-->
<!--		</td>-->
<!--		</tr>-->

		<tr align="left">
		<td>
			<font size="4">Code</font>
		</td>
		<td>
			<font size="4">: [ <a href="https://github.com/Mhaiyang/2023_IJCV_OPNet" target="_blank">Github</a> ] </font>
		</td>
		</tr>

		</table>
</div>


<br>
<h4><p><font size="5" color="black"><b>3. BibTex</b></font></p></h4>
<!--<hr/>-->
<font size="3">
@InProceedings{Haiyang:OPNet:2023,<br>
&nbsp;&nbsp;&nbsp;&nbsp;author = {Mei, Haiyang and Xu, Ke and Zhou, Yunduo and Wang, Yang and Piao, Haiyin and Wei, Xiaopeng and Yang, Xin.},<br>
&nbsp;&nbsp;&nbsp;&nbsp;title = {Camouflaged Object Segmentation with Omni Perception},<br>
&nbsp;&nbsp;&nbsp;&nbsp;booktitle = {International Journal of Computer Vision (IJCV)},<br>
&nbsp;&nbsp;&nbsp;&nbsp;month = {June},<br>
&nbsp;&nbsp;&nbsp;&nbsp;year = {2023}<br>
}
</font>


<br>
<h4><p><font size="5" color="black"><b>4. Website Visit Statistics</b></font></p></h4>
<!--<br><br>-->
<!--<h2>Website visit statistics</h2>-->
<!--<hr/>-->
<script type="text/javascript" src="//rf.revolvermaps.com/0/0/1.js?i=5bcu07xubpc&amp;s=220&amp;m=0&amp;v=true&amp;r=false&amp;b=000000&amp;n=false&amp;c=ff0000" async="async"></script>


<!--<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/emailjs-com@2/dist/email.min.js"></script>-->
<!--<script type="text/javascript">-->
<!--   (function(){-->
<!--      emailjs.init("user_objBJSsyEdqmhf8JaUiLu");-->
<!--   })();-->
<!--</script>-->
<!--<script type="text/javascript" src="https://mhaiyang.github.io/CVPR2023_E2P/main.js"></script>-->

</body>

</html>
