<!DOCTYPE html>
<html>
<head>
<title>TCSVT2021-DCENet</title>

<style media="screen" type="text/css">
body
{
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}
body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 1080px;
  font-family: Times New Roman, Lato, Verdana, Helvetica, sans-serif;
  font-size: 14px;
  background: #fdfdfd;
}
</style>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?1ec4ad5c61857459aa78d5ee7ddee28d";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


</head>

<body>
<h3 align="center"><i><font size="3" face="Palatino Linotype">IEEE Transactions on Circuits and Systems for Video Technology (TCSVT) 2021</font></i></h3>

<table align="center">
<td align="center">
<h1>Exploring Dense Context for Salient Object Detection</h1>
<h3>
	<a href="http://mhaiyang.github.io" target="_blank"><font size="3"><b>Haiyang Mei</b></font></a><sup><font size="2">1</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
	<font size="3"><b>Yuanyuan Liu</b></font><sup><font size="2">1</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
	<font size="3"><b>Ziqi Wei</b></font><sup><font size="2">2,*</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
	<font size="3"><b>Dongsheng Zhou</b></font><sup><font size="2">3</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
	<font size="3"><b>Xiaopeng Wei</b></font><sup><font size="2">1</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
	<font size="3"><b>Qiang Zhang</b></font><sup><font size="2">1</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
	<a href="http://faculty.dlut.edu.cn/yangxin/zh_CN/index/949121/list/index.htm" target="_blank"><font size="3"><b>Xin Yang</b></font></a><sup><font size="2">1,*</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
</h3>


<sup><font size="2">1</font></sup>
<b><a><font size="3">Dalian University of Technology</font></a></b>&nbsp;&nbsp;&nbsp;&nbsp;
<sup><font size="2">2</font></sup>
<b><a><font size="3">Tsinghua University</font></a></b>&nbsp;&nbsp;&nbsp;&nbsp;
<sup><font size="2">3</font></sup>
<b><a><font size="3">Dalian University</font></a></b>&nbsp;&nbsp;&nbsp;&nbsp;

<br>
<br>&nbsp;
<!--<sup><font size="2">&dagger;</font></sup>-->
<!--<sup><font size="3">*</font></sup>-->
<!--<a><font size="3"> Corresponding author</font></a>-->
<!--<br>-->
<!--<br>&nbsp;-->
	<b><a><font size="3"> Contact us:&nbsp;&nbsp;&nbsp;&nbsp;<i>xinyang@dlut.edu.cn&nbsp;&nbsp;&nbsp;&nbsp;mhy666@mail.dlut.edu.cn</i></font></a></b>
</td>
</table>


<br><br>
<table align="center">
<tr>
	<td align="center"><img border=0 height=250 width=1200 src="DCENet.png"></td>
</tr>
</table>

<!--<br>-->
<!--<table align="center">-->
<!--<tr>-->
	<!--<td align="center"><img border=0 height=350 width=800 src="Glass.gif"></td>-->
<!--</tr>-->
<!--</table>-->

<br>
<h2><p><font size="6"><b>Abstract</b></font></p></h2>
<hr/>
<p><font size="4" face="Palatino Linotype">Contexts play an important role in salient object detection (SOD). High-level contexts describe the relations between different parts/objects and thus are helpful for discovering the specific locations of salient objects while low-level contexts could provide the fine detail information for delineating the boundary of the salient objects. However, the way of perceiving/leveraging rich contexts has not been fully investigated by existing SOD works. The common context extraction strategies (\emph{e.g.}, leveraging convolutions with large kernels or atrous convolutions with large dilation rates) do not consider the effectiveness and efficiency simultaneously and may cause sub-optimal solutions. In this paper, we devote to exploring an effective and efficient way to learn rich contexts for accurate SOD. Specifically, we first build a dense context exploration (DCE) module to capture dense multi-scale contexts and further leverage the learned contexts to enhance the features discriminability. Then, we embed multiple DCE modules in an encoder-decoder architecture to harvest dense contexts of different levels. Furthermore, we propose an attentive skip-connection to transmit useful features from the encoder part to the decoder part for better dense context exploration. Finally, extensive experiments demonstrate that the proposed method achieves more superior detection results on the six benchmark datasets than 18 state-of-the-art SOD methods.
</font></p>


<!--<br>-->
<!--<h2><p><font size="6"><b>DCENet</b></font></p></h2>-->
<!--<hr/>-->
<!--<table align="center">-->
<!--<tr>-->
	<!--&lt;!&ndash;<td align="center"><img border=0 height=270 width=960 src="pipeline.png"></td>&ndash;&gt;-->
<!--</tr>-->
<!--</table>-->

<br>
<h2><p><font size="6"><b>Visual Results</b></font></p></h2>
<hr/>
<table align="center">
<tr>
	<td align="center"><img border=0 height=600 width=1060 src="visual.png"></td>
</tr>
</table>


<br>
<h2><p><font size="6"><b>Downloads</b></font></p></h2>
<hr/>
<div align="left">
		<table>
		<tr align="left">
		<td>
			<font size="4">Paper</font>
		</td>
		<td>
			<font size="4">: [ <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9389751" target="_blank">DCENet.pdf</a> ]</font>
		</td>
		</tr>

		<tr align="left">
		<td>
			<font size="4">Experimental Results</font>
		</td>
		<td>
			<font size="4">: [ <a href="https://drive.google.com/file/d/1WBIrgWXDFRfOAxYGWbxNj5ObbpLpCLU3/view?usp=sharing" target="_blank">Google Drive</a> ]</font>
			<font size="4">[ <a href="https://pan.baidu.com/s/1S-PYP-ycJ3C-3dzt0wnoBw" target="_blank">Baidu Disk</a>, fetch cod: pdau ]</font>
		</td>

		</tr>

		<tr align="left">
		<td>
			<font size="4">Pre-trained Model</font>
		</td>
		<td>
			<font size="4">: [ <a href="https://drive.google.com/file/d/1Oym6J7bEvIZcS90braOXUWSNVOUcPeER/view?usp=sharing" target="_blank">Google Drive</a> ]</font>
			<font size="4">[ <a href="https://pan.baidu.com/s/1E6i74_jxKePz0NlYsRiWxA" target="_blank">Baidu Disk</a>, fetch cod: vefs ]</font>
		</td>
		</tr>

		<tr align="left">
		<td>
			<font size="4">Source Code</font>
		</td>
		<td>
			<font size="4">: [ <a href="https://github.com/Mhaiyang/TCSVT2021_DCENet" target="_blank">Code</a> ]</font>
		</td>
		</tr>

		<tr align="left">
		<td>
			<font size="4">Datasets</font>
		</td>
		<td>
			<font size="4">: [ <a href="https://pan.baidu.com/s/1XumHLEqE8Cdg6_v-cgGfFg" target="_blank">Baidu Disk</a>, fetch cod: jj6n ]</font>
		</td>
		</tr>

		</table>
</div>
<br>
<br>



<h2><p><font size="6" color="black"><b>BibTex</b></font></p></h2>
<hr/>
<font size="3">
@article{Mei_2021_TCSVT,<br>
&nbsp;&nbsp;&nbsp;&nbsp;author = {Mei, Haiyang and Liu, Yuanyuan and Wei, Ziqi and Zhou, Dongsheng and Wei, Xiaopeng and Zhang, Qiang and Yang, Xin},<br>
&nbsp;&nbsp;&nbsp;&nbsp;title = {Exploring Dense Context for Salient Object Detection},<br>
&nbsp;&nbsp;&nbsp;&nbsp;booktitle = {IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)},<br>
&nbsp;&nbsp;&nbsp;&nbsp;year = {2021}<br>
}
</font>


<br><br>
<h2>Website visit statistics</h2>
<hr/>
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=400&t=tt&d=0JOX15dnOh8Ta2LNxncvRRDXzD4nW6HsPkiKhyGwjhE'></script>


</body>

</html>
