<!DOCTYPE html>
<html>
<head>
<title>Haiyang Mei</title>

    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

<style media="screen" type="text/css">
body
{
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}
body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 1080px;
  font-family: Times New Roman, Lato, Verdana, Helvetica, sans-serif;
  font-size: 14px;
  background: #fdfdfd;
}
</style>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?1ec4ad5c61857459aa78d5ee7ddee28d";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

</head>

<body>
		
	<table align="center">
	<tr>
	<td align="center"><img border=0 height=275 width=550 src="meihaiyang.jpg"></td>
	<td align="center">&nbsp</td>
	<td align="center">&nbsp</td>
	<td align="center">&nbsp</td>
	<td align="center">&nbsp</td>
	<td align="center">
			<td align="center"><h2>Haiyang Mei</h2>
		<p><font size=+1><b>梅海洋</b><br><br>Research Fellow<br>National University of Singapore</font></p>
		<p><font size=+1>Email: <i>haiyang.mei@outlook.com</i></font><br></p>
		<p><font size=+1>Wechat: mhy845879017</font><br></p>
			</td>			
	</td>
	<td align="right">&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp</td>

	</tr>
	</table>

	<br>
	<h2>Biography</h2>
	<hr/>
<!--	<p><font size="4">I am currently a last-year Ph.D. student in the <a href="http://cs.dlut.edu.cn/">School of Computer Science and Technology</a>, <a href="https://www.dlut.edu.cn/">Dalian University of Technology</a>, supervised by Prof Xiaopeng Wei and <a href="https://xinyangdut.github.io/" target="_blank">Prof Xin Yang</a>, and also received academic guidance from <a href="http://www.cs.cityu.edu.hk/~rynson/" target="_blank">Rynson W.H. Lau</a> and <a href="https://dongshuhao.github.io/" target="_blank">Bo Dong</a>. In 2022, I was a joint Ph.D. student of Dalian University of Technology, China, and Institute of Neuroinformatics, University of Zurich and ETH Zurich, Switzerland, supervised by Prof. <a href="https://www.ini.uzh.ch/~tobi/" target="_blank">Tobi Delbruck</a>. Before that, I received the B.Eng. degree from the School of Control Science and Engineering, <a href="https://www.dlut.edu.cn/">Dalian University of Technology.</a> in 2017.</font></p>-->
	<p><font size="4">
		I am currently a Research Fellow at <a href="https://sites.google.com/view/showlab/home?authuser=0" target="_blank">Show Lab @ NUS</a>, National University of Singapore, under the supervision of Prof. <a href="https://sites.google.com/view/showlab/home?authuser=0" target="_blank">Mike Zheng Shou</a>. Before that, I received my Ph.D. from <a href="https://www.dlut.edu.cn/">Dalian University of Technology</a>, supervised by Prof. Xiaopeng Wei and <a href="https://xinyangdut.github.io/" target="_blank">Prof. Xin Yang</a>, co-supervised by <a href="http://www.cs.cityu.edu.hk/~rynson/" target="_blank">Rynson W.H. Lau</a> and <a href="https://dongshuhao.github.io/" target="_blank">Bo Dong</a>. I was a visting Ph.D. student from Jan 2022 to Jan 2023 in <a href="http://sensors.ini.uzh.ch/home.html" target="_blank">Sensors Group</a>, <a href="https://www.ini.uzh.ch/en.html" target="_blank">Institute of Neuroinformatics</a>, University of Zurich and ETH Zurich, Switzerland, supervised by Prof. <a href="https://www.ini.uzh.ch/~tobi/" target="_blank">Tobi Delbruck</a>.
	</font></p>
	<br>
	
	<h2>Research Interests</h2>
	<hr/>
	<p><font size="4">My primary research interest is in designing effective visual understanding models for the vision systems. This work can help AI agents in scene-level understanding, reasoning, and decision making.
		<br>
		<br>
		My recent endeavor is on <font color="red">scene confusing-discovery</font>, which aims to mine the confusing/special yet meaningful object/region in the scene. In particular, the objects I focus on include <font color="blue">glass</font>, <font color="blue">mirror</font>, <font color="blue">camouflaged object</font>, and <font color="blue">salient object</font>, which are very common in daily life scenes but can confuse the vision systems due to their inherently special properties. Therefore, detecting and segmenting such objects from the scene plays an essential role in accurate scene understanding and can benefit a wide range of computer vision, graphics, and multimedia applications, including image classification, visual tracking, content-aware image editing, medical image diagnosis, and robotic navigation. However, this task has not been fully explored and remains an unsolved and challenging problem. Both glass and mirror do not have their own visual appearances but only transmit/reflect the appearances of their surroundings, making them fundamentally different from other common objects that have been addressed well by the state-of-the-art segmentation methods. The camouflaged/salient object is the object that is ``seamlessly'' embedded in their surroundings or most attention-grabbing and could be easily cluttered by the background in the complex scenes. I am working toward exploring useful cues and effective methods for accurate segmentation.
		<br>
		<br>
		Besides, my early work was on <font color="red">image super-resolution</font>, which is to reconstruct the high-quality, visually satisfactory high-resolution image from the input low-resolution one and is the cornerstone of providing more detailed information for scene analysis and understanding.
	</font></p>
	<br>

	<h2>News!</h2>
	<hr/>
	<ul>
		<li><font size="4">2025.12: One paper was accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34" target="_blank">TPAMI</a> ! </font></li>
		<li><font size="4">2025.11: One paper was accepted by <a href="https://aaai.org/conference/aaai/aaai-26/" target="_blank">AAAI 2026</a> ! </font></li>
		<li><font size="4">2025.09: One paper was accepted by <a href="https://neurips.cc/Conferences/2025" target="_blank">NeurIPS 2025</a> ! </font></li>
		<li><font size="4">2025.07: One paper was accepted by <a href="https://acmmm2025.org/" target="_blank">ACM MM 2025</a> ! </font></li>
		<li><font size="4">2025.02: One paper was accepted by <a href="https://cvpr.thecvf.com/Conferences/2025" target="_blank">CVPR 2025</a> ! </font></li>
		<li><font size="4">2024.09: Two papers were accepted by <a href="https://neurips.cc/Conferences/2024" target="_blank">NeurIPS 2024</a> ! </font></li>
		<li><font size="4">2023.12: One paper was accepted by <a href="https://aaai.org/aaai-conference/" target="_blank">AAAI 2024</a> ! </font></li>
		<li><font size="4">2023.07: One paper was accepted by <a href="https://www.acmmm2023.org/" target="_blank">ACM MM 2023</a> ! </font></li>
		<li><font size="4">2023.06: <b>Got my Ph.D. degree from DUT and please call me Dr. Mei !</b></font></li>
		<li><font size="4">2023.04: One paper was accepted by <a href="https://www.sciengine.com/SSI/home?slug=abstracts&abbreviated=scp" target="_blank">中国科学：信息科学 2023</a> ! </font></li>
		<li><font size="4">2023.04: One paper was accepted by <a href="https://www.springer.com/journal/11263/?gclid=CjwKCAjwkMeUBhBuEiwA4hpqEEGrkq-0KVFbnr7uRn6oyUV9PDcpaUMLhZnswmJ-F6h6COO71Buy0BoCicQQAvD_BwE" target="_blank">IJCV 2023</a> ! </font></li>
		<li><font size="4">2023.02: One paper was accepted by <a href="https://cvpr2023.thecvf.com/" target="_blank">CVPR 2023</a> ! </font></li>
		<li><font size="4">2022.09: One paper was accepted by <a href="https://dl.acm.org/journal/tomm" target="_blank">ACM TOMM 2022</a> ! </font></li>
		<li><font size="4">2022.05: One paper was accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34" target="_blank">IEEE TPAMI 2022</a> ! </font></li>
		<li><font size="4">2022.03: One paper was accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83" target="_blank">IEEE TIP 2022</a> !</font></li>
		<li><font size="4">2022.03: One paper was accepted by <a href="http://cvpr2022.thecvf.com/" target="_blank">CVPR 2022</a> ! </font></li>
		<li><font size="4">2021.03: One paper was accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76" target="_blank">IEEE TCSVT 2021</a> !</font></li>
		<li><font size="4">2021.03: One paper was accepted by <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76" target="_blank">IEEE TCSVT 2021</a> !</font></li>
		<li><font size="4">2021.03: Two papers were accepted by <a href="http://cvpr2021.thecvf.com/" target="_blank">CVPR 2021</a> ! (One Oral)</font></li>
		<li><font size="4">2020.03: One paper was accepted by <a href="https://www.2020.ieeeicme.org/" target="_blank">ICME 2020</a> !</font></li>
		<li><font size="4">2020.02: One paper was accepted by <a href="http://cvpr2020.thecvf.com/" target="_blank">CVPR 2020</a> !</font></li>
	</ul>
	<br>
	
	<h2>Publications &nbsp<a href="https://scholar.google.com/citations?user=yfq6OSkAAAAJ&hl=en" target="_blank">[Google Scholar]</a></h2>
	<hr/>
	<table>
	<tbody>
		<tr>
			<td><font size="4"><b>2025</b></font></td>
		</tr>

		<tr>
			<td><font size="4">1.</font></td>
			<td><center><img width="220" height="135" src="2025_CVPR/teaser.png"></center></td>
			<td><font size="4">SAM-I2V: Upgrading SAM to Support Promptable Video Segmentation with Less than 0.2% Training Cost
				<br>
				<b>Haiyang Mei</b>, Pengyu Zhang, Mike Zheng Shou.
				<br>
				<i>The IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR)</b> 2025</i>
				<br>
				[<b><a href="https://arxiv.org/pdf/2506.01304" target="_blank">arXiv</a></b>|<b><a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Mei_SAM-I2V_Upgrading_SAM_to_Support_Promptable_Video_Segmentation_with_Less_CVPR_2025_paper.pdf" target="_blank">Open Access</a></b>|<a href="https://github.com/showlab/SAM-I2V" target="_blank"><b>Project Page</b></a>]
			</font></td>
		</tr>

		<tr></tr>
		<tr></tr>

		<tr>
			<td><font size="4">2.</font></td>
			<td><center><img width="260" height="100" src="2025_MM/teaser.jpg"></center></td>
			<td><font size="4">Can I Trust You? Advancing GUI Task Automation with Action Trust Score
				<br>
				<b>Haiyang Mei</b>, Difei Gao, Xiaopeng Wei, Xin Yang, Mike Zheng Shou.
				<br>
				<i>The 33rd ACM International Conference on Multimedia (<b>ACM MM)</b> 2025</i>
				<br>
				[<b><a href="https://dl.acm.org/doi/10.1145/3746027.3755618" target="_blank">Open Access</a></b>|<a href="https://github.com/showlab/TrustScorer" target="_blank"><b>Project Page</b></a>]
<!--				[<b>arXiv</b>[<b>Open Access</b>|<b>Project Page</b>]-->
			</font></td>
		</tr>

		<tr></tr>
		<tr></tr>

		<tr>
			<td><font size="4">3.</font></td>
			<td><center><img width="260" height="85" src="2025_NeurIPS/teaser1.png"></center></td>
			<td><font size="4">You Only Communicate Once: One-shot Federated Low-Rank Adaptation of MLLM
				<br>
				Binqian Xu, <b>Haiyang Mei</b>, Zechen Bai, Jinjin Gong, Rui Yan, Guo-Sen Xie, Yazhou Yao, Basura Fernando, Xiangbo Shu.
				<br>
				<i>The Thirty-ninth Annual Conference on Neural Information Processing Systems (<b>NeurIPS</b>) 2025</i>
				<br>
				[<b>PDF</b>|<b>Project Page</b>]
			</font></td>
		</tr>

		<tr></tr>
		<tr></tr>

		<tr>
			<td><font size="4">4.</font></td>
			<td><center><img width="260" height="100" src="2025_arxiv/teaser1.png"></center></td>
			<td><font size="4">InterFeedback: Unveiling Interactive Intelligence of Large Multimodal Models via Human Feedback
				<br>
				Henry Hengyuan Zhao, Wenqi Pei, Yifei Tao, <b>Haiyang Mei</b>, Mike Zheng Shou.
				<br>
<!--				<i>The Thirteenth International Conference on Learning Representations Workshops (<b>ICLRW</b>) 2025</i>-->
				<i>The 2025 Conference on Empirical Methods in Natural Language Processing (<b>EMNLP</b>) 2025</i>
				<br>
				[<b><a href="https://arxiv.org/pdf/2502.15027" target="_blank">PDF</a></b>|<a href="" target="_blank"><b>Project Page</b></a>]
			</font></td>
		</tr>

		<tr></tr>
		<tr></tr>

		<tr>
			<td><font size="4">5.</font></td>
			<td><center><img width="260" height="90" src="2025_arxiv/teaser2.png"></center></td>
			<td><font size="4">FedMLLM: Federated Fine-tuning MLLM on Multimodal Heterogeneity Data
				<br>
				Binqian Xu, Xiangbo Shu, <b>Haiyang Mei</b>, Guosen Xie, Basura Fernando, Mike Zheng Shou, Jinhui Tang.
				<br>
				<i>arXiv:2411.14717</i>
				<br>
				[<b><a href="https://arxiv.org/pdf/2411.14717v2" target="_blank">PDF</a></b>|<a href="https://github.com/1xbq1/FedMLLM" target="_blank"><b>Project Page</b></a>]
			</font></td>
		</tr>

		<tr>
			<td><font size="4">6.</font></td>
			<td><center><img width="220" height="140" src="2026_AAAI/teaser.png"></center></td>
			<td><font size="4">View-on-Graph: Zero-Shot 3D Visual Grounding via Vision-Language Reasoning on Scene Graphs
				<br>
				Yuanyuan Liu, <b>Haiyang Mei</b>, Dongyang Zhan, Jiayue Zhao, Dongsheng Zhou, Bo Dong, Xin Yang.
				<br>
				<i>The 40th Annual AAAI Conference on Artificial Intelligence (<b>AAAI</b>) 2026</i>
				<br>
				[<b><a href="https://arxiv.org/pdf/2512.09215v1" target="_blank">PDF</a></b>|<a href="https://github.com/YYLiuDLUT/VoG" target="_blank"><b>Project Page</b></a>]
			</font></td>
		</tr>

		<tr>
			<td><br><br><br><font size="4"><b>2024</b></font></td>
		</tr>

		<tr>
			<td><font size="4">1.</font></td>
			<td><center><img width="300" height="90" src="2024_ICLRW/teaser.png"></center></td>
			<td><font size="4">Skip \n: A Simple Method to Reduce Hallucination in Large Vision-Language Models
				<br>
				Zongbo Han, Zechen Bai, <b>Haiyang Mei</b>, Qianli Xu, Changqing Zhang, Mike Zheng Shou.
				<br>
				<i>The Twelfth International Conference on Learning Representations Workshops (<b>ICLRW</b>) 2024</i>
				<br>
				[<b><a href="https://arxiv.org/pdf/2402.01345v1.pdf" target="_blank">PDF</a></b>|<a href="https://arxiv.org/html/2402.01345v1#:~:text=We%20have%20validated%20this%20hypothesis,output%20of%20%27%5Cn%27." target="_blank"><b>Project Page</b></a>]
			</font></td>
		</tr>

		<tr></tr>
		<tr></tr>

		<tr>
			<td><font size="4">2.</font></td>
			<td><center><img width="300" height="90" src="2024_IJCAI/teaser.png"></center></td>
			<td><font size="4">Apprenticeship-Inspired Elegance: Synergistic Knowledge Distillation Empowers Spiking Neural Networks for Efficient Single-Eye Emotion Recognition
				<br>
				Yang Wang, <b>Haiyang Mei</b>, Qirui Bao, Ziqi Wei, Mike Zheng Shou, Haizhou Li, <br>Bo Dong, Xin Yang.
				<br>
				<i>The 33rd International Joint Conference on Artificial Intelligence (<b>IJCAI</b>) 2024</i>
				<br>
				[<b><a href="https://www.ijcai.org/proceedings/2024/0350.pdf" target="_blank">PDF</a></b>|<a href="" target="_blank"><b>Project Page</b></a>]
			</font></td>
		</tr>

		<tr></tr>
		<tr></tr>

		<tr>
			<td><font size="4">3.</font></td>
			<td><center><img width="300" height="60" src="2024_NeurIPS/teaser1.png"></center></td>
			<td><font size="4">One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos
				<br>
				Zechen Bai, Tong He, <b>Haiyang Mei</b>, Pichao Wang, Ziteng Gao, Joya Chen, liulei, Zheng Zhang, Mike Zheng Shou.
				<br>
				<i>The Thirty-eighth Annual Conference on Neural Information Processing Systems (<b>NeurIPS</b>) 2024</i>
				<br>
				[<b><a href="https://arxiv.org/pdf/2409.19603v1" target="_blank">PDF</a></b>|<a href="https://github.com/showlab/VideoLISA" target="_blank"><b>Project Page</b></a>]
<!--				[<b>PDF</b>|<b>Project Page</b>]-->
			</font></td>
		</tr>

		<tr></tr>
		<tr></tr>

		<tr>
			<td><font size="4">4.</font></td>
			<td><center><img width="300" height="100" src="2024_NeurIPS/teaser2.png"></center></td>
			<td><font size="4">DoFIT: Domain-aware Federated Instruction Tuning with Alleviated Catastrophic Forgetting
				<br>
				Binqian Xu, Xiangbo Shu, <b>Haiyang Mei</b>, Zechen Bai, Basura Fernando, Mike Zheng Shou, Jinhui Tang.
				<br>
				<i>The Thirty-eighth Annual Conference on Neural Information Processing Systems (<b>NeurIPS</b>) 2024</i>
				<br>
				[<b><a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/9be9407431b7ff8cc04cae5460fcb7ab-Paper-Conference.pdf" target="_blank">PDF</a></b>|<a href="https://github.com/1xbq1/DoFIT" target="_blank"><b>Project Page</b></a>]
<!--				[<b>PDF</b>|<b>Project Page</b>]-->
			</font></td>
		</tr>

		<tr></tr>
		<tr></tr>

		<tr>
			<td><font size="4">5.</font></td>
			<td><center><img width="300" height="90" src="2024_TIM/teaser2.png"></center></td>
			<td><font size="4">Steel Sheet Counting from an Image with a Two-Stream Network
				<br>
				Zhiling Cui, <b>Haiyang Mei</b>, Wen Dong, Ziqi Wei, Zheng Lv, Dongsheng Zhou, Xin Yang.
				<br>
				<i>IEEE Transactions on Instrumentation & Measurement (<b>TIM</b>) 2024</i>
				<br>
				[<b><a href="https://ieeexplore.ieee.org/document/10887007" target="_blank">PDF</a></b>|<a href="" target="_blank"><b>Project Page</b></a>]
<!--				[<b>PDF</b>|<b>Project Page</b>]-->
			</font></td>
		</tr>

		<tr>
			<td><br><br><br><font size="4"><b>2023</b></font></td>
		</tr>

		<tr></tr>
		<tr></tr>
		<tr></tr>

		<tr>
			<td><font size="4">1.</font></td>
			<td><center><img width="300" height="150" src="CVPR2023_E2P/teaser.png"></center></td>
			<td><font size="4">Deep Polarization Reconstruction with PDAVIS Events
				<br>
				<b>Haiyang Mei</b>, Zuowen Wang, Xin Yang, Xiaopeng Wei, Tobi Delbruck.
				<br>
				<i>The IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) 2023</i>
				<br>
				[<b><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Mei_Deep_Polarization_Reconstruction_With_PDAVIS_Events_CVPR_2023_paper.html">PDF</a></b>|<a href="CVPR2023_E2P/index.html" target="_blank"><b>Project Page</b></a>]
<!--				[<b><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Mei_Glass_Segmentation_Using_Intensity_and_Spectral_Polarization_Cues_CVPR_2022_paper.html" target="_blank">PDF</a></b>|<a href="CVPR2022_PGSNet/index.html" target="_blank"><b>Project Page</b></a>]-->
<!--				[<b>PDF</b>|<b>Project Page</b>]-->
			</font></td>
		</tr>

		<tr></tr>
		<tr></tr>
		<tr></tr>


		<tr>
			<td><font size="4">2.</font></td>
			<td><center><img width="300" height="130" src="IJCV2023-OPNet/teaser.png"></center></td>
			<td><font size="4">Camouflaged Object Segmentation with Omni Perception
				<br>
				<b>Haiyang Mei</b>, Ke Xu, Yunduo Zhou, Yang Wang, Haiyin Piao, Xiaopeng Wei, Xin Yang.
				<br>
				<i>International Journal of Computer Vision (<b>IJCV</b>) 2023 </i>
				<br>
				[<b><a href="https://link.springer.com/article/10.1007/s11263-023-01838-2">PDF</a> </b>|<a href="IJCV2023-OPNet/index.html" target="_blank"><b>Project Page</b></a>]
<!--				[<b><a href="https://ieeexplore.ieee.org/document/9748016" target="_blank">PDF</a></b>|<a href="TIP2022-PGSNet/index.html" target="_blank"><b>Project Page</b></a>]-->
<!--				[<b>PDF</b>|<b>Project Page</b>]-->
			</font></td>
		</tr>

		<tr></tr>
		<tr></tr>
		<tr></tr>

		<tr>
			<td><font size="4">3.</font></td>
			<td><center><img width="300" height="200" src="SSI2023-PFNet-Plus/teaser.png"></center></td>
			<td><font size="4">Distraction-Aware Camouflaged Object Segmentation
				<br>
				<b>Haiyang Mei</b>, Xin Yang, Yunduo Zhou, Ge-Peng Ji, Xiaopeng Wei, Deng-Ping Fan.
				<br>
				<i>SCIENTIA SINICA Informationis (<b>SSI</b>) 2023</i>
				<br>
				[<b><a href="https://www.sciengine.com/SSI/doi/10.1360/SSI-2022-0138">PDF</a> </b>|<a href="SSI2023-PFNet-Plus/index.html" target="_blank"><b>Project Page</b></a>]
<!--				[<b><a href="https://ieeexplore.ieee.org/document/9863431" target="_blank">PDF</a></b>|<a href="TPAMI2022-GDNet-B/index.html" target="_blank"><b>Project Page</b></a>]-->
<!--				[<b>PDF</b>|<b>Project Page</b>]-->
			</font></td>
		</tr>

		<tr></tr>
		<tr></tr>
		<tr></tr>

		<tr>
			<td><font size="4">4.</font></td>
			<td><center><img width="300" height="150" src="2023_CADCG/teaser.jpg"></center></td>
			<td><font size="4">A Method for Generating Adversarial Patterns in Facial Recognition with Visual Camouflage
				<br>
				Qirui Bao, <b>Haiyang Mei</b>, Huilin Wei, Zheng Lv, Yuxin Wang, Erwei Yin, Xin Yang.
				<br>
				<i>CADCG 2023</i>
				<br>
				[<b><a href="">PDF</a> </b>|<a href="2023_CADCG/index.html" target="_blank"><b>Project Page</b></a>]
<!--				[<b><a href="https://ieeexplore.ieee.org/document/9863431" target="_blank">PDF</a></b>|<a href="TPAMI2022-GDNet-B/index.html" target="_blank"><b>Project Page</b></a>]-->
<!--				[<b>PDF</b>|<b>Project Page</b>]-->
			</font></td>
		</tr>

		<tr></tr>
		<tr></tr>
		<tr></tr>

		<tr>
			<td><font size="4">5.</font></td>
			<td><center><img width="300" height="130" src="2023_MM_EEM-SAN/teaser.jpg"></center></td>
			<td><font size="4">Event-Enhanced Multi-Modal Spiking Neural Network for Dynamic Obstacle Avoidance
				<br>
				Yang Wang, Bo Dong, Yuji Zhang, Yunduo Zhou, <b>Haiyang Mei</b>, Ziqi Wei, Xin Yang.
				<br>
				<i>ACM International Conference on Multimedia (<b>MM</b>) 2023</i>
				<br>
				[<b><a href="">PDF</a> </b>|<a href="2023_MM_EEM-SAN/index.html" target="_blank"><b>Project Page</b></a>]
<!--				[<b><a href="https://ieeexplore.ieee.org/document/9863431" target="_blank">PDF</a></b>|<a href="TPAMI2022-GDNet-B/index.html" target="_blank"><b>Project Page</b></a>]-->
<!--				[<b>PDF</b>|<b>Project Page</b>]-->
			</font></td>
		</tr>

		<tr></tr>
		<tr></tr>
		<tr></tr>

		<tr>
			<td><font size="4">6.</font></td>
			<td><center><img width="300" height="160" src="2023_CVPRW_Live_Demo/teaser.jpg"></center></td>
			<td><font size="4">Live Demo: E2P–Events to Polarization Reconstruction from PDAVIS Events
				<br>
				Tobi Delbruck, Zuowen Wang, <b>Haiyang Mei</b>, Germain Haessig, Damien Joubert, Justin Haque, Yingkai Chen, Moritz B. Milde, Viktor Gruev.
				<br>
				<i>The IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (<b>CVPRW</b>) 2023</i>
				<br>
				[<b><a href="https://openaccess.thecvf.com/content/CVPR2023W/EventVision/html/Delbruck_Live_Demo_E2P-Events_to_Polarization_Reconstruction_From_PDAVIS_Events_CVPRW_2023_paper.html">PDF</a> </b>|<a href="2023_CVPRW_Live_Demo/index.html" target="_blank"><b>Project Page</b></a>]
<!--				[<b><a href="https://ieeexplore.ieee.org/document/9863431" target="_blank">PDF</a></b>|<a href="TPAMI2022-GDNet-B/index.html" target="_blank"><b>Project Page</b></a>]-->
<!--				[<b>PDF</b>|<b>Project Page</b>]-->
			</font></td>
		</tr>

		<tr></tr>
		<tr></tr>
		<tr></tr>

		<tr>
			<td><font size="4">7.</font></td>
			<td><center><img width="300" height="240" src="2024_AAAI/teaser.png"></center></td>
			<td><font size="4">Exploiting Polarized Material Cues for Robust Car Detection
				<br>
				Wen Dong, <b>Haiyang Mei</b>, Ziqi Wei, Ao Jin, Sen Qiu, Qiang Zhang, Xin Yang.
				<br>
				<i>Thirty-Eighth AAAI Conference on Artificial Intelligence (<b>AAAI</b>) 2024</i>
				<br>
				[<b><a href="https://arxiv.org/abs/2401.02606">PDF</a> </b>|<a href="https://wind1117.github.io/publication/2024-AAAI-PolarCar" target="_blank"><b>Project Page</b></a>]
			</font></td>
		</tr>

		<tr>
			<td><br><br><br><font size="4"><b>2022</b></font></td>
		</tr>

		<tr></tr>
		<tr></tr>
		<tr></tr>

		<tr>
			<td><font size="4">1.</font></td>
			<td><center><img width="300" height="100" src="CVPR2022_PGSNet/teaser.png"></center></td>
			<td><font size="4">Glass Segmentation using Intensity and Spectral Polarization Cues
				<br>
				<b>Haiyang Mei</b>, Bo Dong, Wen Dong, Jiaxi Yang, Seung-Hwan Baek, Felix Heide, Pieter Peers, Xiaopeng Wei, Xin Yang.
				<br>
				<i>The IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) 2022</i>
				<br>
<!--				[<b><a href="">PDF</a></b>|<a href="CVPR2022_PGSNet/index.html" target="_blank"><b>Project Page</b></a>]-->
				[<b><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Mei_Glass_Segmentation_Using_Intensity_and_Spectral_Polarization_Cues_CVPR_2022_paper.html" target="_blank">PDF</a></b>|<a href="CVPR2022_PGSNet/index.html" target="_blank"><b>Project Page</b></a>]
<!--				[<b>PDF</b>|<b>Project Page</b>]-->
			</font></td>
		</tr>

		<tr></tr>
		<tr></tr>
		<tr></tr>

		<tr>
			<td><font size="4">2.</font></td>
			<td><center><img width="300" height="125" src="TIP2022-PGSNet/teaser.png"></center></td>
			<td><font size="4">Progressive Glass Segmentation
				<br>
				Letian Yu<font size="3" color="DeepSkyBlue ">*</font>, <b>Haiyang Mei</b><font size="3" color="DeepSkyBlue ">*</font>, Wen Dong, Ziqi Wei, Li Zhu, Yuxin Wang, Xin Yang. (<font size="3" color="DeepSkyBlue ">*</font> <font size="3">joint first authors</font>)
				<br>
				<i>IEEE Transactions on Image Processing (<b>TIP</b>) 2022 </i>
				<br>
<!--				[<b><a href="">PDF</a> </b>|<a href="TIP2022-PGSNet/index.html" target="_blank"><b>Project Page</b></a>]-->
				[<b><a href="https://ieeexplore.ieee.org/document/9748016" target="_blank">PDF</a></b>|<a href="TIP2022-PGSNet/index.html" target="_blank"><b>Project Page</b></a>]
<!--				[<b>PDF</b>|<b>Project Page</b>]-->
			</font></td>
		</tr>

		<tr></tr>
		<tr></tr>
		<tr></tr>

		<tr>
			<td><font size="4">3.</font></td>
			<td><center><img width="300" height="200" src="TPAMI2022-GDNet-B/application.jpg"></center></td>
			<td><font size="4">Large-Field Contextual Feature Learning for Glass Detection
				<br>
				<b>Haiyang Mei</b>, Xin Yang, Letian Yu, Qiang Zhang, Xiaopeng Wei, Rynson W.H. Lau.
				<br>
				<i>IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>) 2022</i>
				<br>
<!--				[<b><a href="https://ieeexplore.ieee.org/document/9863431">PDF</a></b>|<a href="CVPR2022_PGSNet/index.html" target="_blank"><b>Project Page</b></a>]-->
				[<b><a href="https://ieeexplore.ieee.org/document/9863431" target="_blank">PDF</a></b>|<a href="TPAMI2022-GDNet-B/index.html" target="_blank"><b>Project Page</b></a>]
<!--				[<b>PDF</b>|<b>Project Page</b>]-->
			</font></td>
		</tr>

		<tr></tr>
		<tr></tr>
		<tr></tr>

		<tr>
			<td><font size="4">4.</font></td>
			<td><center><img width="300" height="160" src="TOMM2022-MirrorNet+/tomm_teaser.png"></center></td>
			<td><font size="4">Mirror Segmentation via Semantic-Aware Contextual Contrasted Feature Learning
				<br>
				<b>Haiyang Mei</b>, Letian Yu, Ke Xu, Yang Wang, Xin Yang, Xiaopeng Wei, Rynson W.H. Lau.
				<br>
				<i>ACM Transactions on Multimedia Computing, Communications, and Applications (<b>TOMM</b>) 2022</i>
				<br>
				[<b><a href="https://dl.acm.org/doi/full/10.1145/3566127">PDF</a></b>|<a href="TOMM2022-MirrorNet+/index.html" target="_blank"><b>Project Page</b></a>]
<!--				[<b>PDF</b>|<a href="TOMM2022-MirrorNet+/index.html" target="_blank"><b>Project Page</b></a>]-->
<!--				[<b>PDF</b>|<b>Project Page</b>]-->
			</font></td>
		</tr>

		<tr>
			<td><br><br><br><font size="4"><b>2021</b></font></td>
		</tr>

		<tr></tr>
		<tr></tr>
		<tr></tr>

		<tr>
			<td><font size="4">1.</font></td>
			<td><center><img width="300" height="145" src="pdnet_teaser.png"></center></td>
			<td><font size="4">Depth-Aware Mirror Segmentation
				<br>
				<b>Haiyang Mei</b>, Bo Dong, Wen Dong, Pieter Peers, Xin Yang, Qiang Zhang, Xiaopeng Wei.
				<br>
				<i>The IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) 2021 (<font color="purple"><b>Oral</b></font>)</i>
				<br>
				[<b><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Mei_Depth-Aware_Mirror_Segmentation_CVPR_2021_paper.pdf">PDF</a> </b>|<a href="CVPR2021_PDNet/index.html" target="_blank"><b>Project Page</b></a>]
			</font></td>
		</tr>

		<tr></tr>
		<tr></tr>
		<tr></tr>

		<tr>
			<td><font size="4">2.</font></td>
			<td><center><img width="300" height="125" src="pfnet_teaser.png"></center></td>
			<td><font size="4">Camouflaged Object Segmentation with Distraction Mining
				<br>
				<b>Haiyang Mei</b>, Ge-Peng Ji, Ziqi Wei, Xin Yang, Xiaopeng Wei, Deng-Ping Fan.
				<br>
				<i>The IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) 2021</i>
				<br>
				[<b><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Mei_Camouflaged_Object_Segmentation_With_Distraction_Mining_CVPR_2021_paper.pdf">PDF</a></b>|<a href="CVPR2021_PFNet/index.html" target="_blank"><b>Project Page</b></a>]
			</font></td>
		</tr>

		<tr></tr>
		<tr></tr>
		<tr></tr>

		<tr>
			<td><font size="4">3.</font></td>
			<td><center><img width="300" height="130" src="dcenet_teaser.png"></center></td>
			<td><font size="4">Exploring Dense Context for Salient Object Detection
				<br>
				<b>Haiyang Mei</b>, Yuanyuan Liu, Ziqi Wei, Dongsheng Zhou, Xiaopeng Wei, Qiang Zhang, Xin Yang.
				<br>
				<i>IEEE Transactions on Circuits and Systems for Video Technology (<b>TCSVT</b>) 2021</i>
				<br>
				[<b><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9389751">PDF</a></b>|<a href="TCSVT2021-DCENet/index.html" target="_blank"><b>Project Page</b></a>]
			</font></td>
		</tr>

		<tr></tr>
		<tr></tr>
		<tr></tr>

		<tr>
			<td><font size="4">4.</font></td>
			<td><center><img width="300" height="220" src="tsan_teaser.png"></center></td>
			<td><font size="4">A Two-Stage Attentive Network for Single Image Super-Resolution
				<br>
				Jiqing Zhang, Chengjiang Long, Yuxin Wang, Haiyin Piao, <b>Haiyang Mei</b>, Xin Yang, Baocai Yin.
				<br>
				<i>IEEE Transactions on Circuits and Systems for Video Technology (<b>TCSVT</b>) 2021</i>
				<br>
				[<b><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9395490">PDF</a></b>|<a href="TCSVT2021-TSAN/index.html" target="_blank"><b>Project Page</b></a>]
			</font></td>
		</tr>

		<tr>
			<td><br><br><br><font size="4"><b>2020</b></font></td>
		</tr>

		<tr>
			<td><font size="4">1.</font></td>
			<td><center><img width="300" height="240" src="glass_teaser.png"></center></td>
			<td><font size="4">Don't Hit Me! Glass Detection in Real-World Scenes
				<br>
				<b>Haiyang Mei</b>, Xin Yang, Yang Wang, Yuanyuan Liu, Shengfeng He,
				<br>
				Qiang Zhang, Xiaopeng Wei, Rynson W.H. Lau.
				<br>
				<i>The IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) 2020</i>
				<br>
				[<b><a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Mei_Dont_Hit_Me_Glass_Detection_in_Real-World_Scenes_CVPR_2020_paper.pdf" target="_blank">PDF</a></b>|<a href="CVPR2020_GDNet/index.html" target="_blank"><b>Project Page</b></a>]
			</font></td>
		</tr>

		<tr></tr>
		<tr></tr>
		<tr></tr>

		<tr>
			<td><font size="4">2.</font></td>
			<td><center><img width="300" height="130" src="icme_teaser.png"></center></td>
			<td><font size="4">Multi-Context And Enhanced Reconstruction Network For Single Image Super Resolution
				<br>
				Jiqing Zhang, Chengjiang Long, Yuxin Wang, Xin Yang, <b>Haiyang Mei</b>, Baocai Yin.
				<br>
				<i>IEEE International Conference on Multimedia and Expo (<b>ICME</b>) 2020</i>
				<br>
				[<b><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9102868" target="_blank">PDF</a></b>|<a href="ICME2020_MCERN/index.html" target="_blank"><b>Project Page</b></a>]
			</font></td>
		</tr>


		<tr>
			<td><br><br><br><font size="4"><b>2019</b></font></td>
		</tr>

		<tr>
			<td><font size="4">1.</font></td>
			<td><center><img width="300" src="ICCV2019_MirrorNet/teaser.jpg"></center></td>
			<td><font size="4">Where Is My Mirror?
				<br>
				Xin Yang<font size="3" color="DeepSkyBlue ">*</font>, <b>Haiyang Mei</b><font size="3" color="DeepSkyBlue ">*</font>, Ke Xu, Xiaopeng Wei, Baocai Yin, Rynson Lau. (<font size="3" color="DeepSkyBlue ">*</font> <font size="3">joint first authors</font>)
				<br>
				<i>IEEE International Conference on Computer Vision (<b>ICCV</b>) 2019</i>
				<br>
				[<b><a href="https://arxiv.org/pdf/1908.09101v2.pdf" target="_blank">PDF</a></b>|<a href="ICCV2019_MirrorNet/index.html" target="_blank"><b>Project Page</b></a>]
			</font></td>
		</tr>

		<tr>
			<td><br><br><br><font size="4"><b>2018</b></font></td>
		</tr>

		<tr>
			<td><font size="4">1.</font></td>
			<td><center><img width="300" src="tmm_teaser.jpg"></center></td>
			<td><font size="4">DRFN: Deep Recurrent Fusion Network for Single Image Super-Resolution with Large Factors
				<br>
				Xin Yang, <b>Haiyang Mei</b>, Jiqing Zhang, Ke Xu, Baocai Yin, Qiang Zhang, Xiaopeng Wei.
				<br>
				<i>IEEE Transactions on Multimedia (<b>TMM</b>) 2018</i>
				<br>
			[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8425771" target="_blank"><b>PDF</b></a>|
				<a href="TMM2018-DRFN/index.html" target="_blank"><b>Project Page</b></a>]
			</font></td>
		</tr>
		
	</tbody>
	</table>
	<br>

	<h2>Patent</h2>
	<hr/>
	<table>
	<tr>
		<td><font size="3"><b>1.</b> &nbsp 一种利用回归树场的图像超分辨率放大方法. &nbsp 201710859709.0</font></td>
	</tr>

	<tr>
		<td><font size="3"><b>2.</b> &nbsp 基于信息挖掘的伪装目标图像分割方法. &nbsp CN112750140B</font></td>
	</tr>

	<tr>
		<td><font size="3"><b>3.</b> &nbsp 基于深度感知的镜子图像分割方法. &nbsp CN112767418B</font></td>
	</tr>

	<tr>
		<td><font size="3"><b>4.</b> &nbsp 一种真实场景下玻璃检测的方法. &nbsp CN111339917B</font></td>
	</tr>

	<tr>
		<td><font size="3"><b>5.</b> &nbsp 一种基于多任务协作的镜子检测方法. &nbsp CN111339919B</font></td>
	</tr>

	<tr>
		<td><font size="3"><b>6.</b> &nbsp Method for Glass Detection in Real Scenes. &nbsp US011361534B2</font></td>
	</tr>

	</table>

	<br><br>
	<h2>Website visit statistics</h2>
	<hr/>
	<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=400&t=tt&d=YVHaD32jOtcB3Eiq9UMYVAERx0kd9bogZFwa-ljWvqw&co=2d78ad&cmo=3acc3a&cmn=ff5353&ct=ffffff'></script>


</body>

</html>
